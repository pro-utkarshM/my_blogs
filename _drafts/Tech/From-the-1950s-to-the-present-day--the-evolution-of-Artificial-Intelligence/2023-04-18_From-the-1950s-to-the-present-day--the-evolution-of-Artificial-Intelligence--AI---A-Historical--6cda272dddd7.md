---
title: "From the 1950s to the present day, the evolution of Artificial
  Intelligence (AI): A Historical..."
---

<div>

# From the 1950s to the present day, the evolution of Artificial Intelligence (AI): A Historical... {#from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical .p-name}

</div>

::: {.section .p-summary field="subtitle"}
\[Part 5\] From the 1950s to the present day, the evolution of
Artificial Intelligence (AI)
:::

::: {.section .e-content field="body"}
::: {#4f01 .section .section .section--body .section--first .section--last}
::: section-divider

------------------------------------------------------------------------
:::

::: section-content
::: {.section-inner .sectionLayout--insetColumn}
### From the 1950s to the present day, the evolution of Artificial Intelligence (AI): A Historical Account. (Part 5) {#e04c .graf .graf--h3 .graf--leading .graf--title name="e04c"}

**Here are the milestones we'll cover:**

> [The Dartmouth
> Conference](https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-65f9b59adb02){.markup--anchor
> .markup--blockquote-anchor
> data-href="https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-65f9b59adb02"
> rel="noopener" target="_blank"}\
> [The
> Perceptron](https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-450b4a449b74){.markup--anchor
> .markup--blockquote-anchor
> data-href="https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-450b4a449b74"
> rel="noopener" target="_blank"}\
> [The AI boom of the 1960s\
>  The AI winter of the
> 1980s](https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-e718369a8ba6){.markup--anchor
> .markup--blockquote-anchor
> data-href="https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-e718369a8ba6"
> target="_blank"}\
>  The Development of Expert Systems\
>  The Emergence of NLPs and Computer Vision in the 1990s\
>  The Rise of Big Data\
>  The Advent of Deep Learning\
>  The Development of Generative AI

### The Development of Expert Systems {#1768 .graf .graf--h3 .graf-after--blockquote name="1768"}

Expert systems are a type of artificial intelligence (AI) technology
that was developed in the 1980s. Expert systems are designed to mimic
the decision-making abilities of a human expert in a specific domain or
field, such as medicine, finance, or engineering.

During the 1960s and early 1970s, there was a lot of optimism and
excitement around AI and its potential to revolutionise various
industries. But as we discussed in the past section, this enthusiasm was
dampened by the AI winter, which was characterised by a lack of progress
and funding for AI research.

The development of expert systems marked a turning point in the history
of AI. Pressure on the AI community had increased along with the demand
to provide practical, scalable, robust, and quantifiable applications of
Artificial Intelligence.

Expert systems served as proof that AI systems could be used in real
life systems and had the potential to provide significant benefits to
businesses and industries. Expert systems were used to automate
decision-making processes in various domains, from diagnosing medical
conditions to predicting stock prices.

In technical terms, expert systems are typically composed of a knowledge
base, which contains information about a particular domain, and an
inference engine, which uses this information to reason about new inputs
and make decisions. Expert systems also incorporate various forms of
reasoning, such as deduction, induction, and abduction, to simulate the
decision-making processes of human experts.

Overall, expert systems were a significant milestone in the history of
AI, as they demonstrated the practical applications of AI technologies
and paved the way for further advancements in the field.

Today, expert systems continue to be used in various industries, and
their development has led to the creation of other AI technologies, such
as machine learning and natural language processing.

### The Emergence of NLPs and Computer Vision in the 1990s {#a73b .graf .graf--h3 .graf-after--p name="a73b"}

During the 1990s, AI research and globalization began to pick up some
momentum. This period ushered in the modern era of Artificial
Intelligence reserach.

As discussed in the previous section, expert systems came into play
around the late 1980s and early 1990s. But they were limited by the fact
that they relied on structured data and rules-based logic. They
struggled to handle unstructured data, such as natural language text or
images, which are inherently ambiguous and context-dependent.

To address this limitation, researchers began to develop techniques for
processing natural language and visual information.

In the 1970s and 1980s, significant progress was made in the development
of rule-based systems for NLP and Computer Vision. But these systems
were still limited by the fact that they relied on pre-defined rules and
were not capable of learning from data.

In the 1990s, advances in machine learning algorithms and computing
power led to the development of more sophisticated NLP and Computer
Vision systems.

Researchers began to use statistical methods to learn patterns and
features directly from data, rather than relying on pre-defined rules.
This approach, known as machine learning, allowed for more accurate and
flexible models for processing natural language and visual information.

One of the most significant milestones of this era was the development
of the [Hidden Markov
Model](https://en.wikipedia.org/wiki/Hidden_Markov_model){.markup--anchor
.markup--p-anchor
data-href="https://en.wikipedia.org/wiki/Hidden_Markov_model"
rel="noopener" target="_blank"} (HMM), which allowed for probabilistic
modeling of natural language text. This resulted in significant advances
in speech recognition, language translation, and text classification.

Similarly, in the field of Computer Vision, the emergence of
Convolutional Neural Networks (CNNs) allowed for more accurate object
recognition and image classification.

These techniques are now used in a wide range of applications, from
self-driving cars to medical imaging.

Overall, the emergence of NLP and Computer Vision in the 1990s
represented a major milestone in the history of AI. They allowed for
more sophisticated and flexible processing of unstructured data.

These techniques continue to be a focus of research and development in
AI today, as they have significant implications for a wide range of
industries and applications.

### The Rise of Big Data {#06f6 .graf .graf--h3 .graf-after--p name="06f6"}

The concept of big data has been around for decades, but its rise to
prominence in the context of artificial intelligence (AI) can be traced
back to the early 2000s. Before we dive into how it relates to AI, let's
briefly discuss the term Big Data.

For data to be termed *big*, it needs to fulfill 3 core attributes:
Volume, Velocity, and Variety.

Volume refers to the sheer size of the data set, which can range from
terabytes to petabytes or even larger.

Velocity refers to the speed at which the data is generated and needs to
be processed. For example, data from social media or IoT devices can be
generated in real-time and needs to be processed quickly.

And variety refers to the diverse types of data that are generated,
including structured, unstructured, and semi-structured data.

Before the emergence of big data, AI was limited by the amount and
quality of data that was available for training and testing machine
learning algorithms.

[Natural language processing
(NLP)](https://www.freecodecamp.org/news/learn-natural-language-processing-no-experience-required/){.markup--anchor
.markup--p-anchor
data-href="https://www.freecodecamp.org/news/learn-natural-language-processing-no-experience-required/"
rel="noopener" target="_blank"} and [computer
vision](https://www.freecodecamp.org/news/how-to-use-tensorflow-for-computer-vision/){.markup--anchor
.markup--p-anchor
data-href="https://www.freecodecamp.org/news/how-to-use-tensorflow-for-computer-vision/"
rel="noopener" target="_blank"} were two areas of AI that saw
significant progress in the 1990s, but they were still limited by the
amount of data that was available.

For example, early NLP systems were based on hand-crafted rules, which
were limited in their ability to handle the complexity and variability
of natural language.

The rise of big data changed this by providing access to massive amounts
of data from a wide variety of sources, including social media, sensors,
and other connected devices. This allowed machine learning algorithms to
be trained on much larger datasets, which in turn enabled them to learn
more complex patterns and make more accurate predictions.

At the same time, advances in data storage and processing technologies,
such as Hadoop and Spark, made it possible to process and analyze these
large datasets quickly and efficiently. This led to the development of
new machine learning algorithms, such as deep learning, which are
capable of learning from massive amounts of data and making highly
accurate predictions.

Today, big data continues to be a driving force behind many of the
latest advances in AI, from autonomous vehicles and personalised
medicine to natural language understanding and recommendation systems.

As the amount of data being generated continues to grow exponentially,
the role of big data in AI will only become more important in the years
to come.

### The Advent of Deep Learning {#e2c7 .graf .graf--h3 .graf-after--p name="e2c7"}

The emergence of [Deep
Learning](https://en.wikipedia.org/wiki/Deep_learning){.markup--anchor
.markup--p-anchor
data-href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener"
target="_blank"} is a major milestone in the globalisation of modern
Artificial Intelligence.

Ever since the Dartmouth Conference of the 1950s, AI has been recognised
as a legitimate field of study and the early years of AI research
focused on symbolic logic and rule-based systems. This involved manually
programming machines to make decisions based on a set of predetermined
rules. While these systems were useful in certain applications, they
were limited in their ability to learn and adapt to new data.

It wasn't until after the rise of big data that deep learning became a
major milestone in the history of AI. With the exponential growth of the
amount of data available, researchers needed new ways to process and
extract insights from vast amounts of information.

Deep learning algorithms provided a solution to this problem by enabling
machines to automatically learn from large datasets and make predictions
or decisions based on that learning.

[Deep learning is a type of machine
learning](https://www.freecodecamp.org/news/deep-learning-crash-course-learn-the-key-concepts-and-terms/){.markup--anchor
.markup--p-anchor
data-href="https://www.freecodecamp.org/news/deep-learning-crash-course-learn-the-key-concepts-and-terms/"
rel="noopener" target="_blank"} that uses artificial neural networks,
which are modeled after the structure and function of the human brain.
These networks are made up of layers of interconnected nodes, each of
which performs a specific mathematical function on the input data. The
output of one layer serves as the input to the next, allowing the
network to extract increasingly complex features from the data.

One of the key advantages of deep learning is its ability to learn
hierarchical representations of data. This means that the network can
automatically learn to recognise patterns and features at different
levels of abstraction.

For example, a deep learning network might learn to recognise the shapes
of individual letters, then the structure of words, and finally the
meaning of sentences.

The development of deep learning has led to significant breakthroughs in
fields such as computer vision, speech recognition, and natural language
processing. For example, deep learning algorithms are now able to
accurately classify images, recognise speech, and even generate
realistic human-like language.

Deep learning represents a major milestone in the history of AI, made
possible by the rise of big data. Its ability to automatically learn
from vast amounts of information has led to significant advances in a
wide range of applications, and it is likely to continue to be a key
area of research and development in the years to come.

> ***We appreciate your interest in this ongoing topic and thank you for
> staying updated. Please stay tuned for our upcoming articles where we
> will share details of each milestone.***
:::
:::
:::
:::

By [Utkarsh Maurya](https://medium.com/@sankalp.1519){.p-author .h-card}
on [April 18, 2023](https://medium.com/p/6cda272dddd7).

[Canonical
link](https://medium.com/@sankalp.1519/from-the-1950s-to-the-present-day-the-evolution-of-artificial-intelligence-ai-a-historical-6cda272dddd7){.p-canonical}

Exported from [Medium](https://medium.com) on November 25, 2023.
